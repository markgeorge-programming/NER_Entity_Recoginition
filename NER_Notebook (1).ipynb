{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94a19108"
      },
      "source": [
        "<div style=\"background-color: orange; padding: 10px; border-radius: 5px; text-align: center;box-shadow:8px 8px 8px 8px;color:white;\">\n",
        "  <h1 style=\"color: black;\">Natural Entity Recoginition model (NER) </h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Nfl_x4IQWjf"
      },
      "source": [
        "# Introduction:\n",
        "This notebook demonstrates the implementation of a **Hybrid Natural Language Processing (NLP) Architecture** designed for high-precision entity extraction in e-commerce environments (e.g., brand names, specific SKUs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSgEkQMFQWjg"
      },
      "source": [
        "#### We are building an AI model that:\n",
        "- Reads a sentence word by word\n",
        "- Understands the context of each word\n",
        "- Assigns a label (tag) to every word\n",
        "\n",
        "This task is called Named Entity Recognition (NER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aOygoq_QWjg"
      },
      "source": [
        "# 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Kguu0RKkYMdv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "#stop the tensorflow warnings since we run the model training on cpu\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yngNSa74QWji"
      },
      "source": [
        "<div style=\"background-color: cyan; padding: 10px; border-radius: 5px; text-align: center;box-shadow:8px 8px 8px 8px;color:white;\">\n",
        "  <h1 style=\"color: black;\">Preprocessing </h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y-xfmcrQWji"
      },
      "source": [
        "# 2. Initalising the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YldKy6lQWjj"
      },
      "source": [
        "### Dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v3PErzIQWjk"
      },
      "source": [
        "The dataset was given in sentences to simulate given raw data for ai to proccess and train on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4D_ALe4rYJwy"
      },
      "outputs": [],
      "source": [
        "\n",
        "#These sentneces are for training the AI model\n",
        "sentences = [\n",
        "    \"grab a black coat today in paris\",\n",
        "    \"I would like a pink sneakers tomorrow in cairo\",\n",
        "    \"show me a purple hat on friday in london\",\n",
        "    \"looking for blue tshirt on wednesday in Egypt\",\n",
        "\n",
        "    \"grab a yellow coat today\",\n",
        "    \"I would like a gray sneakers tomorrow\",\n",
        "    \"show me a white hat on friday\",\n",
        "    \"looking for orange tshirt on wednesday\",\n",
        "\n",
        "    \"grab a black coat in paris\",\n",
        "    \"I would like a red sneakers in cairo\",\n",
        "    \"show me a green hat in london\",\n",
        "    \"looking for brown tshirt in Egypt\",\n",
        "\n",
        "    \"give me a pink chocolate\",\n",
        "    \"I would like a gold sneakers\",\n",
        "    \"red hoodie is cool\",\n",
        "    \"blue shirt for sale\"\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMZYfxlZQWjl"
      },
      "source": [
        "### Labeling the data set:\n",
        "\n",
        "* 0 -> O (unknown)\n",
        "* 1 -> color\n",
        "* 2 -> Product\n",
        "* 3 -> Date\n",
        "* 4 -> Location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "C_AWPDRkV5dq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "labels = [\n",
        "    [0,0,1,2,3,0,4],\n",
        "    [0,0,0,0,1,2,3,0,4],\n",
        "    [0,0,0,1,2,0,3,0,4],\n",
        "    [0,0,1,2,0,3,0,4],\n",
        "\n",
        "    [0,0,1,2,3],\n",
        "    [0,0,0,0,1,2,3],\n",
        "    [0,0,0,1,2,0,3],\n",
        "    [0,0,1,2,0,3],\n",
        "\n",
        "    [0,0,1,2,0,4],\n",
        "    [0,0,0,0,1,2,0,4],\n",
        "    [0,0,0,1,2,0,4],\n",
        "    [0,0,1,2,0,4],\n",
        "\n",
        "    [0,0,0,1,2],\n",
        "    [0,0,0,0,1,2],\n",
        "    [1, 2, 0, 0],\n",
        "    [1, 2, 0, 0]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSiFYqpaQWjl"
      },
      "source": [
        "# 3. Tokenizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5Z1ExNaQWjl"
      },
      "source": [
        "We setup a tokenizer to seperate the sentences into an array of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztc6kTy_P5AD",
        "outputId": "5941d41c-7fda-448e-a5a3-34df73eed196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Data Ready. Vocab Size: 44\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Setup Tokenizer\n",
        "tokenizer = Tokenizer(oov_token=\"OOV\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1  # +1 for padding\n",
        "\n",
        "# Convert Text to Sequences (Integers)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Padding (Make all sentences same length for the Matrix Math)\n",
        "max_length = 20\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "padded_labels = pad_sequences(labels, maxlen=max_length, padding='post')\n",
        "\n",
        "# One-Hot Encode Labels for the Softmax Layer\n",
        "# (0 -> [1,0,0], 1 -> [0,1,0], 2 -> [0,0,1])\n",
        "Y = to_categorical(padded_labels, num_classes=5)\n",
        "\n",
        "print(f\" Data Ready. Vocab Size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toMYznerQWjm"
      },
      "source": [
        "<div style=\"background-color: cyan; padding: 10px; border-radius: 5px; text-align: center;box-shadow:8px 8px 8px 8px;color:white;\">\n",
        "  <h1 style=\"color: black;\">Model training </h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSmFNXb0QWjm"
      },
      "source": [
        "# 4. Model Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3gS4Oi2QWjn"
      },
      "source": [
        "The model is using the following layers:\n",
        "* Embedding layers: Turns words into a dense vector of 16 numbers\n",
        "\n",
        "* Bidirectional Layers:Understands the context of sentences by detecting if a label would likely come after another label\n",
        "\n",
        "* Dropout: Stops a precentage of the neurons in the neural network to prevent overfitting\n",
        "\n",
        "* Time distributed Dense Softmax layer: The classifier of the words using a softmax activation for multivariable logistic regression, TimeDistributed is used so the classifier applies for each words rather than entire sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx3jw5JYQWjn",
        "outputId": "439d8f80-4cfd-4169-a477-9146f3965ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training the model...\n",
            " Training Complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=16, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(5, activation='softmax'))\n",
        "\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(\"\\n Training the model...\")\n",
        "model.fit(padded_sequences, Y, epochs=300, verbose=0)\n",
        "print(\" Training Complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwcDArEMQWjn"
      },
      "source": [
        "<div style=\"background-color: cyan; padding: 10px; border-radius: 5px; text-align: center;box-shadow:8px 8px 8px 8px;color:white;\">\n",
        "  <h1 style=\"color: black;\">Model Prediction </h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9eaT-fBQWjn"
      },
      "source": [
        "# 5. Model Prediction test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u79mbEfj2Tpa",
        "outputId": "447bff5b-88aa-4393-b529-662f895a3483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”® PREDICTION TEST:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468ms/step\n",
            "ðŸ“ Input: I wear blue sneakers\n",
            "------------------------------\n",
            "Word: 'i' \t-> Tag: O \t(Conf: 0.99)\n",
            "Word: 'OOV' \t-> Tag: O \t(Conf: 0.62)\n",
            "Word: 'blue' \t-> Tag: COLOR \t(Conf: 0.80)\n",
            "Word: 'sneakers' \t-> Tag: PRODUCT \t(Conf: 0.92)\n"
          ]
        }
      ],
      "source": [
        "# --- STEP 5: FIELD TEST (Prediction) ---\n",
        "print(\"\\nðŸ”® PREDICTION TEST:\")\n",
        "test_sentence = [\"I wear blue sneakers\"]\n",
        "test_seq = tokenizer.texts_to_sequences(test_sentence)\n",
        "test_padded = pad_sequences(test_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "prediction = model.predict(test_padded)\n",
        "predicted_ids = np.argmax(prediction, axis=-1)[0]\n",
        "\n",
        "# Decode the Output\n",
        "id_to_word = {v: k for k, v in word_index.items()}\n",
        "tag_map = {\n",
        "    0: \"O\",\n",
        "    1: \"COLOR\",\n",
        "    2: \"PRODUCT\",\n",
        "    3: \"DATE\",\n",
        "    4: \"LOCATION\"\n",
        "}\n",
        "\n",
        "print(f\"ðŸ“ Input: {test_sentence[0]}\")\n",
        "print(\"-\" * 30)\n",
        "for i, word_id in enumerate(test_padded[0]):\n",
        "    if word_id != 0: # Skip padding\n",
        "        word = id_to_word[word_id]\n",
        "        tag = tag_map[predicted_ids[i]]\n",
        "        # Show the Probability confidence (Your \"Stats\" Brain)\n",
        "        confidence = np.max(prediction[0][i])\n",
        "        print(f\"Word: '{word}' \\t-> Tag: {tag} \\t(Conf: {confidence:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHT6SXrhQWjo"
      },
      "source": [
        "## Save model for deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZLNrl9i4jU4",
        "outputId": "acbbf2ab-d021-47e2-bb65-3813d80fa38c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model.save('ner_high_accuracy.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}